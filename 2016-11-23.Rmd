---
title: "STA257"
author: "Neil Montgomery | HTML is official | PDF versions good for in-class use only"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  ioslides_presentation: 
    css: '../styles.css' 
    widescreen: true 
    transition: 0.001
header-includes:
- \usepackage{cancel}
---
\newcommand\given[1][]{\:#1\vert\:}
\newcommand\P[1]{P{\left(#1\right)}}
\newcommand\F[1]{F_{\tiny{#1}}}
\newcommand\f[1]{f_{\tiny{#1}}}
\newcommand\p[1]{p_{\tiny{#1}}}
\newcommand\V[1]{\text{Var}\!\left(#1\right)}
\newcommand\E[1]{E\!\left(#1\right)}


## $E(g(X))$ and extensions { .build }

Motivation: suppose $X \sim N(\mu, \sigma^2)$. What is $E(X)?$ The answer is $\mu$. Lots of ways to figure this out.

Using the density is tedious but do-able. Or we could use the fact that $X = \mu + \sigma Z$ with $Z \sim N(0,1)$.

Theorem: Given $X$ and $E(X)$ exists, consider $g(x) = a + bx$. Then $E(g(X)) = E(a + bX) = a + bE(X)$.

Proof: ...

## $E(g(X))$

A theorem which is too difficult to prove generally is: given $X$, any* $g$, and $Y = g(X)$, then:
$$E(Y) = E(g(X)) = \begin{cases}
\sum g(x)p(x) &: X \text{ discrete}\\\\
\int g(x)f(x)\,dx &: X \text{ continuous}
\end{cases}$$
in both cases provided the sum/integral congerges "absolutely" (i.e. with $|g(x)|.$)

Example: Average volume of sphere with radius $R \sim \text{Exp}(1)$...

## $E(g(X_1,\ldots,X_n))$ { .build }

Some typical applications:

$$E(X_1\cdot X_2)$$

$$E\left(X_1 + X_2\right)$$

$$E(X_1 + \cdots + X_n)$$

$$E\left(\overline{X}\right) = E\left(\frac{X_1 + \cdots + X_n}{n}\right)$$

Theorem (continuous version): $X_1,\ldots,X_n$ have joint density $f(x_1,\ldots,x_n)$ and $Y = g(X_1,\ldots,X_n)$. Then:
$$E(Y) = \int\ldots\int g(x_1,\ldots,x_n) f(x_1,\ldots,x_n)\,dx_1\ldots\,dx_n$$

## examples { .smaller }

Suppose $X_1 \perp X_2.$ Consider $E(X_1\cdot X_2)$...

Exercise: $X_1 \perp X_2.$ Consider $\E{g(X_1)\, h(X_2)}$

Now suppose $X_1,\ldots,X_n$ are i.i.d. with $E(X_i) = \mu.$ Consider:
$$E\left(\overline{X}\right) = E\left(\frac{X_1 + \cdots + X_n}{n}\right)=\mu$$
**Note added after class: Try the proof with $n=2$. Notice that there was nothing special about $n$. Also notice that the only requirement for $E\left(\overline{X}\right) = \mu$ is just that $E(X_i) = \mu$. Try for $n=2$ just with a general joint distribution and no independence and not the same distribution for $X_1$ and $X_2$ and only $E(X_1)=E(X_2) = \mu$**

$X\sim \text{NegBin}(r,p)$...
